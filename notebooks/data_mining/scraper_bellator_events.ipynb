{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Bellator_MMA_events\"\n",
    "\n",
    "soup = BeautifulSoup(\n",
    "    requests.get(url).text,\n",
    "    \"html.parser\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all(\"table\", class_ = \"sortable\")[0]\n",
    "table_rows = table.find_all(\"tr\")[1:]\n",
    "anchors = [_.find_all(\"a\") for _ in table_rows if len(_.find_all(\"a\"))>0]\n",
    "cols = [\"link\", \"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_df = pd.DataFrame([(anchor[0].get(\"href\"), anchor[0].text) for anchor in anchors], columns=cols)\n",
    "clean_link = lambda x: f\"https://en.wikipedia.org/{x.split('#')[0]}\"\n",
    "links_df[\"link_clean\"] = links_df[\"link\"].apply(clean_link)\n",
    "links = links_df[\"link_clean\"].value_counts().reset_index().iloc[:,0]\n",
    "links = links.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanResults(result):\n",
    "    event_name = result[\"event\"]\n",
    "    df = result[\"df\"]\n",
    "    df.columns = [_[1] for _ in df.columns]\n",
    "\n",
    "    split_by_card = [(i, df.iloc[i,0]) for i in df[df.eq(df.iloc[:, 0], axis=0).all(axis=1)].index]\n",
    "    if 0 not in [_[0] for _ in split_by_card]:\n",
    "        split_by_card = [(0, \"Main card\"), *split_by_card]\n",
    "\n",
    "    for i in range(len(split_by_card)-1):\n",
    "        split_by_card[i] = (split_by_card[i][0], split_by_card[i+1][0], split_by_card[i][1])\n",
    "\n",
    "    split_by_card[-1] = (split_by_card[-1][0], len(df), split_by_card[-1][1])\n",
    "\n",
    "    sdfs = []\n",
    "    for start, end, card in split_by_card:\n",
    "        sdf = df.iloc[start:end, :]\n",
    "        sdf = sdf.assign(fight_card = card)\n",
    "        sdfs.append(sdf)\n",
    "    \n",
    "    df = pd.concat(sdfs).drop([i[0] for i in split_by_card[1:]]).reset_index(drop=True)\n",
    "\n",
    "    cols2rename = {\n",
    "        x: x.lower().replace(' ', '_') for x in df.columns\n",
    "    }\n",
    "\n",
    "    cols2rename = {\n",
    "        **cols2rename,\n",
    "        **{\n",
    "            \"Unnamed: 1_level_1\": \"winner\",\n",
    "            \"Unnamed: 3_level_1\": \"loser\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    df.rename(columns=cols2rename, inplace=True)\n",
    "\n",
    "    df.drop(columns=[\"unnamed:_2_level_1\"], axis=1, inplace=True)\n",
    "\n",
    "    df = df.assign(event_name = event_name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(url):\n",
    "    dfs = []\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "    header_patterns = [\"bellator\"]\n",
    "    pattern_match = lambda x: len([_ for _ in header_patterns if _ in x.lower()]) > 0\n",
    "\n",
    "    # event_headers = soup.find_all(lambda tag: tag.name == \"h2\" and \"one\" in tag.text.lower() and \"cancelled\" not in tag.text.lower())\n",
    "    event_headers = [\n",
    "        header for header in soup.find_all(\n",
    "        lambda tag: \n",
    "        tag.name == \"h2\" and \"cancelled\" not in tag.text.lower() and \"tournament\" not in tag.text.lower()\n",
    "        ) if pattern_match(header.text)\n",
    "        ]\n",
    "\n",
    "    if not event_headers:\n",
    "        event_headers = soup.find_all(lambda tag: tag.name == \"h1\" and \"bellator\" in tag.text.lower() and \"cancelled\" not in tag.text.lower())\n",
    "    \n",
    "    table_classes = [_.get(\"class\") for _ in event_headers[0].find_all_next(\"table\")]\n",
    "\n",
    "    table_class = \"wikitable\" if \"toccolours\" not in [x for xs in list(filter(lambda x: x!=None, table_classes)) for x in xs] else \"toccolours\"\n",
    "    \n",
    "    for event in event_headers:\n",
    "\n",
    "        event_name = event.text.replace(\"[edit]\", \"\")\n",
    "        tables = event.find_next(\"table\", class_ = table_class)\n",
    "        if tables != None:\n",
    "            df = pd.read_html(str(tables))\n",
    "        else:\n",
    "            df = pd.read_html(str(event.find_next(\"table\", class_ = \"wikitable\")))\n",
    "        \n",
    "        df = df[0]\n",
    "        df = cleanResults(result={\"event\": event_name, \"df\": df})\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "16 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "17 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "18 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "19 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "20 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "21 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "22 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "23 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "24 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "25 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "26 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "27 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "28 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "29 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "30 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "31 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "32 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "33 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "34 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "35 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "46 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "47 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "48 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "49 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "50 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "51 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "52 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "53 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "54 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "55 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "56 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "57 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "58 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "59 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "60 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "61 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "62 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "63 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "64 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "65 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "66 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "68 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "95 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "97 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "98 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n",
      "99 https://en.wikipedia.org//wiki/Bellator_MMA_in_2019\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(links):\n",
    "    try:\n",
    "        getData(_)\n",
    "    except:\n",
    "        print(i, links[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m getData(links[\u001b[39m99\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m, in \u001b[0;36mgetData\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     27\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_html(\u001b[39mstr\u001b[39m(tables))\n\u001b[0;32m     28\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_html(\u001b[39mstr\u001b[39;49m(event\u001b[39m.\u001b[39;49mfind_next(\u001b[39m\"\u001b[39;49m\u001b[39mtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, class_ \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mwikitable\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[0;32m     31\u001b[0m df \u001b[39m=\u001b[39m df[\u001b[39m0\u001b[39m]\n\u001b[0;32m     32\u001b[0m df \u001b[39m=\u001b[39m cleanResults(result\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mevent\u001b[39m\u001b[39m\"\u001b[39m: event_name, \u001b[39m\"\u001b[39m\u001b[39mdf\u001b[39m\u001b[39m\"\u001b[39m: df})\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\envs\\ultimateNakMuay\\Lib\\site-packages\\pandas\\io\\html.py:1212\u001b[0m, in \u001b[0;36mread_html\u001b[1;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend)\u001b[0m\n\u001b[0;32m   1208\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m   1210\u001b[0m io \u001b[39m=\u001b[39m stringify_path(io)\n\u001b[1;32m-> 1212\u001b[0m \u001b[39mreturn\u001b[39;00m _parse(\n\u001b[0;32m   1213\u001b[0m     flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[0;32m   1214\u001b[0m     io\u001b[39m=\u001b[39;49mio,\n\u001b[0;32m   1215\u001b[0m     match\u001b[39m=\u001b[39;49mmatch,\n\u001b[0;32m   1216\u001b[0m     header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m   1217\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m   1218\u001b[0m     skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m   1219\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m   1220\u001b[0m     thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m   1221\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1222\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1223\u001b[0m     decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[0;32m   1224\u001b[0m     converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m   1225\u001b[0m     na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m   1226\u001b[0m     keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[0;32m   1227\u001b[0m     displayed_only\u001b[39m=\u001b[39;49mdisplayed_only,\n\u001b[0;32m   1228\u001b[0m     extract_links\u001b[39m=\u001b[39;49mextract_links,\n\u001b[0;32m   1229\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[0;32m   1230\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\envs\\ultimateNakMuay\\Lib\\site-packages\\pandas\\io\\html.py:1001\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[39massert\u001b[39;00m retained \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     \u001b[39mraise\u001b[39;00m retained\n\u001b[0;32m   1003\u001b[0m ret \u001b[39m=\u001b[39m []\n\u001b[0;32m   1004\u001b[0m \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables:\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\envs\\ultimateNakMuay\\Lib\\site-packages\\pandas\\io\\html.py:981\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m p \u001b[39m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[0;32m    980\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 981\u001b[0m     tables \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mparse_tables()\n\u001b[0;32m    982\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m caught:\n\u001b[0;32m    983\u001b[0m     \u001b[39m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[39m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(io, \u001b[39m\"\u001b[39m\u001b[39mseekable\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m io\u001b[39m.\u001b[39mseekable():\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\envs\\ultimateNakMuay\\Lib\\site-packages\\pandas\\io\\html.py:257\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_tables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m     tables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_tables(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_doc(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattrs)\n\u001b[0;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m tables)\n",
      "File \u001b[1;32mc:\\Users\\paulm\\anaconda3\\envs\\ultimateNakMuay\\Lib\\site-packages\\pandas\\io\\html.py:613\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[1;34m(self, doc, match, attrs)\u001b[0m\n\u001b[0;32m    610\u001b[0m tables \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mfind_all(element_name, attrs\u001b[39m=\u001b[39mattrs)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tables:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo tables found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    615\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    616\u001b[0m unique_tables \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "getData(links[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimateNakMuay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
